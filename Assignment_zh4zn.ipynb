{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The source of data: \n",
    "I will be using the SUN20 dataset. \n",
    "\n",
    "## The decision problem:\n",
    "In my project, for example, I want to train a computer to understand an image. \n",
    "To be more specific, by providing data and images as input, we can use computers to recognize objects, people, animals, places, etc. Some application of my model can be used by people who\n",
    "want to make friends, \n",
    "for example, users can upload an image or a photo to our model, then the model will classify and create a label for it. Moreover, it will match people who with the same label.\n",
    "In my classification project, computers or software are supposed to classify scenery from a branch of photos. \n",
    "Thus, the decision-maker is a program. The agent may confront some decisions. Some of them is, for example, which sort of scenery it is. \n",
    "\n",
    "## The decision make's action set $\\mathbb{A}$: \n",
    "A vector with 20 entries. Each entry represents a score with respect to a given \n",
    "image, and $\\sum_{i=1}^{20}a_i = 1$.\n",
    "\n",
    "\n",
    "## Sample space: \n",
    "Training set is a dataset consisting of 60k training images belonging to 10 categories of garments.\n",
    "The images are rescaled to a size of 28x28 pixels\n",
    "\n",
    "## Data generating process:\n",
    "The whole dataset was split into two parts: Training set and validation set. The validation\n",
    "set has 10k images. The classifier will take input vectors x of size  1x28x28=784 , which is the size of the images. The\n",
    "function representing the linear function is: $$ \\hat{a} =softmax(Wx + b)$$，$$\\hat{a}_i = \\frac{exp（Wx + b)}{\\sum_{k=1}^{10}{exp(Wx + b)}}$$\n",
    " To be more specific,\n",
    "$$\\hat{a}_i = W_{i1}x_{i1} + W_{i2}x_{i2} +...+W_{ij}x_{ij} + b_i$$, W and b can be initialized with random values.\n",
    "$W_{ij} \\in \\mathbb{N}(\\theta_1, \\sigma_1), b \\in \\mathbb{N}(\\theta_2, \\sigma_2)$\n",
    "\n",
    "## The parameter space: \n",
    "$\\theta_1, \\theta_2, \\sigma_1, \\sigma_2 \\in \\mathbb{R} X (0, \\infty)$, and P in [0,1] \n",
    "\n",
    "## The decision-maker's prior beliefs:\n",
    "W and b are normally distributed. Based on previous experience, we can select W and b\n",
    "from $\\mathbb{N}(0, 0.01)$\n",
    "\n",
    "The result of classification depends on the number of images. Training the model or program is time-consuming in that the image input should be large enough to produce classification results. \n",
    "If the image sets are small, the model cannot classify images as we expected. It may, for example, classify a mountain as a canyon. However, it is difficult to collect a large amount of images, which needs time and cutting-edge equipment. \n",
    "That is to say, the cost of making wrong decisions is extremely high. After making a wrong decision, researchers need to adjust the module, resample the dataset and change parameters. This is costs realized from poorer decisions.  According to this example, \n",
    "stakes of decision is the cost when our model makes a wrong decision.\n",
    "We may lose money and time because of a poorer decision. But from the decision, we can collect ideas that can optimize our model, making our decision better next time.\n",
    "\n",
    "\n",
    "## The predictive tool: \n",
    "From the predictive tools, we can learn about what will happen in the future based on historical data. \n",
    "In the case of images, our prediction model can potentially tell you the type, the label of a certain image. \n",
    "To generate a useful prediction, we need to know the process of classification. By comparing the result produced by our model, \n",
    "we can get feedback data. This is data you collect when the prediction machine is operating in real situations. \n",
    "Feedback data is often generated from a richer set of environments than training data. To some extent feedback data is more realistic than the data used for training. \n",
    "So, you can improve the accuracy of predictions further with continual training using feedback data. \n",
    "\n",
    "\n",
    "## The process for getting better information. \n",
    "The decision-maker's beliefs about uncertain parameter values can be sharpen\n",
    "with Gradient Descent where we computer derivatives of loss function with respect to W and b. Then we update the parameters\n",
    "according to the following rules:$$w_{ij}=w{ij}-\\lambda \\frac{\\partial \\ell}{\\partial w_{ij}}$$\n",
    "$$b_i = b_i - \\lambda\\frac{\\partial \\ell}{\\partial b_i}$$. \n",
    "\n",
    "## Relevant payoffs: \n",
    "The model will match 2 people in terms of meeting successful or unsuccessful\n",
    "\n",
    "\n",
    "### Loss function: \n",
    "The loss function between the prediction and true label is:$$\\ell(y, \\hat{y})= -\\sum_iy_ilog(\\hat{y}_i)$$\n",
    "Since the true label vector $y$ only has one non-zero entry, we know this sum really has only one term that is non-zero so we will usually use this loss function simply stated as:\n",
    "$$\\ell(label, \\hat{y}) = -log(\\hat{y}_{label})$$.\n",
    "\n",
    "## The rule the decision-maker will use to choose a preferred action: \n",
    "I will use $\\theta \\in (0, 1)$ to represent the potential matches of people. If $\\theta > 0.7$ we\n",
    "can make a prediction that the 2 people can be matched."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-1b855516",
   "language": "python",
   "display_name": "PyCharm (Assignments)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}